{
  "hash": "f2e6064a5938bfb6e7bc087d67b05245",
  "result": {
    "markdown": "---\ntitle: Writing a Twitter Bot in R\nauthor: Enrico Spinielli\ndate: '2018-06-30'\ncategories: [\"R\", \"Maps\"]\nlicense: CC BY-SA\n---\n\n\n\nSince a while I am contemplating the possibility of automatically publishing\non social media some stats and data visualisations from work.\n\nWhen I discovered the nice bot [\\@everytract](https://twitter.com/everytract) by\n[\\@fitnr](https://twitter.com/fitnr) (and a little later the\n[\\@GVAcartografic](https://twitter.com/GVAcartografic)'s\n[#Secciócensal](https://twitter.com/hashtag/Secci%C3%B3censal?src=hash) tweets)\nI decided to try and do a Twitter bot myself in order to see what is possible and\nhow difficult it is.\n\nI thought that combining maps and some Italian data would be a good recipe\nfor having fun and\nso I did set out to use some GIS data from the [Italian National Institute of Statistics (ISTAT)](https://www.istat.it/en/)([\\@istat_en](https://twitter.com/istat_en))\nin order to publish a map for every Italian [comune](https://en.wikipedia.org/wiki/Comune)\n(the basic administrative division in Italy).\n\n\n## The Data\n\nThe relevant GIS data in the form of Shapefiles can be found on ISTAT's web site,\nhttps://www.istat.it/it/archivio/124086.\nI took the most [detailed polygons as for 2016 with WSG84 datum](gis2016wsg84).\n\nI combined the info from ISTAT in an `sf` dataframe which looks like the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n> coms %>% as_tibble()\n# A tibble: 7,998 x 6\n   PRO_COM COMUNE          bb         REGIONE  DEN_CMPRO geometry                               \n     <dbl> <fct>           <list>     <fct>    <fct>     <S3: sfc_MULTIPOLYGON>                 \n 1    1001 Agliè           <S3: bbox> Piemonte Torino    \"list(list(c(7.7826615064871, 7.783045…\n 2    1002 Airasca         <S3: bbox> Piemonte Torino    \"list(list(c(7.48794557658868, 7.48797…\n 3    1003 Ala di Stura    <S3: bbox> Piemonte Torino    \"list(list(c(7.27324227448866, 7.27359…\n 4    1004 Albiano d'Ivrea <S3: bbox> Piemonte Torino    \"list(list(c(7.92507300912979, 7.92533…\n 5    1005 Alice Superiore <S3: bbox> Piemonte Torino    \"list(list(c(7.79782897832822, 7.79843…\n 6    1006 Almese          <S3: bbox> Piemonte Torino    \"list(list(c(7.4348997124695, 7.435239…\n```\n:::\n\n\n`PRO_COM` is the unique numerical code identifying each Italian comune,\n`COMUNE` is the name of the comune,\n`DEN_CMPRO` is the name of the relevant super-entity the comune belongs to (either\nMetropolitan City or a Province),\n`REGIONE` is the name of the region and `geometry` is the simple feature describing the\nboundary of the comune.\n`bb` is the axis aligned bounding box of the comune's polygon and is used to to extract the\nright portion (at the right zoom) of the satellite map.\n\nAll data preparation is documented in\n[`prepare-data.R`](https://github.com/espinielli/italian-comuni-bot/blob/master/prepare-data.R)\nfile in the [Italian Comuni Twitter bot][twitter-bot-repo] repository.\n\n\n## R packages to the Rescue\n\nI found a lot of examples for developing Twitter bots in Python but I wanted to\nwrite it in R which we are starting to use more and more at work.\n\nOf course we all stand on the shoulder of Giants, and there is always somebody who has\nalready done bits and pieces of what you need: [rtweet](https://rtweet.info/)\ndoes all you need as a client for accessing Twitter API, while the usual suspects\n[ggplot2](https://ggplot2.tidyverse.org/), [sf](https://r-spatial.github.io/sf/) and\n[ggmap](https://cran.r-project.org/package=ggmap) cover the maps aspects.\n\nThe setup for Twitter was smooth and I only executed the steps as described in `rtweet`\npackage, so no need to repeat them here.\n\n\n# Maps\n\n## First attempt\n\nThe first map I produced was a simple ggmap/ggplot map:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](comune_ggmap.jpg){width=1050}\n:::\n:::\n\n\nThis is produced by code similar to this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggmap)\nlibrary(rgdal)\n\n# get the data as per Italian Comuni Twitter Bot repo\n# https://github.com/espinielli/italian-comuni-bot/\ncoms <- readRDS(\"data/coms.rds\")\n\n# just get the first one\ncom <- coms %>%\n  dplyr::filter(row_number() == 1) %>%\n  dplyr::slice(1)\n\ncom.sp <- as(com, \"Spatial\")\n\n# extract its bounding box\nbb <- com$bb[[1]]\n\n# centroid\ncentroid <- com %>%\n  st_transform(23032) %>%\n  st_centroid() %>%\n  st_transform(4326) %>%\n  st_coordinates() %>%\n  as_data_frame() %>%\n  `names<-`(c(\"lon\", \"lat\"))\n\n# get Google Map (note: hardcoded zoom level)\nm <- get_map(location = centroid, zoom = 12, maptype = \"satellite\")\n\nterrain <- ggmap(m)\ncom.sp.df <- com.sp %>% fortify\n\n#----------------------------------------------------------------\n# inspired by\n# https://ryanpeek.github.io/2017-11-21-mapping-with-sf-part-3/\n#----------------------------------------------------------------\npg <- terrain +\n  geom_polygon(\n    data = com.sp.df,\n    aes(x = long, y = lat),\n    fill=NA,\n    color=\"yellow\",\n    lwd = 0.4, alpha=0.5) +\n  labs(x = \"Longitude (WGS84)\",\n       y = \"Latitude\",\n       caption = \"Sources: ISTAT (comuni), Google Maps (satellite)\") +\n  ggtitle(\n    label = str_glue(\"{comune} ({id})\",\n                     comune = com$COMUNE,\n                     id = str_pad(com$PRO_COM, 6, pad = \"0\")),\n    subtitle = str_c(com$DEN_CMPRO, com$REGIONE, sep = \", \"))\n```\n:::\n\n\nThere are few \"smelly\" things in the code above:\n\n* the zoom level is hardcoded to 12\n* the map is not cropped to the boundary\n\n\nThe solution above is wrapped (with fix to first bullet, see below)\nin the function `generate_google_map` in the\n[`tweet-comune.R`](https://github.com/espinielli/italian-comuni-bot/blob/master/tweet-comune.R)\nfile in the [Italian Comuni Twitter bot][twitter-bot-repo] repository.\n\n\n\n## Automatic Zoom Calculation\n\nI was surprised to always see examples with hardcoded zoom levels, but in fact the \n`ggmap` package has a helper function `calc_zoom` just for that.\nThe only problem is that it is buggy, but \n[pull request #141](https://github.com/dkahle/ggmap/pull/141) has a proposed fix\nwhich I just saved in a local `calc_zoom` in the repo.\n\nIt is about taking the _minimum_ instead of the maximun of the zoom levels\nin the longitude and latitude direction.\n\n\n## Crop Map to Polygon\n\nI had no idea how to crop the Google Map to the comune's boundary, but Giants exist\nand I have found a nice solution (by [Robin Lovelace](https://www.robinlovelace.net/)) \n[posted on GIS SO](https://gis.stackexchange.com/a/155495/7617).\n\nIt boils down to transforming the Google Map to raster and `mask`ing it with the\ncomune's boundary:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  m.rast <- ggmap_rast(map = m)\n  com.only <- mask(m.rast, com.sp)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](comune_raster.jpg){width=1050}\n:::\n:::\n\n\n\nThis (and bells and whistles) is now wrapped in a function `generate_cropped_map` in the\n[`tweet-comune.R`](https://github.com/espinielli/italian-comuni-bot/blob/master/tweet-comune.R)\nfile in the [Italian Comuni Twitter bot][twitter-bot-repo] repository.\n\n\n\n# Tweet It!\n\nThe logic to tweet the comunes' maps one by one is as follows:\n\n1. find out what was sent last\n2. get the new comune\n3. decide whether to embelish the tweet with refs & Co.\n4. tweet and save some sort of proof of which comune has been dealt with\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set things up for Twitter\ntweet_authorize()\n\n# this is the core of the process:\n# 1. find out what was sent last\n# 2. get the new comune\n# 3. decide whether to embelish the tweet with refs & Co.\n# 4. tweet and save some sort of proof of which comune has been dealt with\n\n# 1. find out what was sent last\nl <- read_file(\"last-tweeted.txt\") %>% as.integer()\nn <- (l + 1) %% lc\n\n# 2. get the new comune\ncom <- coms %>%\n  dplyr::filter(row_number() == n) %>%\n  dplyr::slice(1)\n\n# 3. decide whether to embelish the tweet with refs & Co.\nmsg <- ifelse((n %% 19) == 0,\n              \"Done in #rstats using #ggplot2, #rspatial, #ggmap and #rtweet.\",\n              \"\")\ncredits <- ifelse((n %% 67) == 0,\n                  \"Sources @istat_it, @istat_en, @googlemaps.\",\n                  \"\")\n\n# 3. decide whether to embelish the tweet with refs & Co.\ntweet_comune(com, n, msg = msg, credits = credits)\n\n# 4. tweet and save some sort of proof of which comune has been dealt with\nwriteLines(text = as.character(n) , \"last-tweeted.txt\")\n```\n:::\n\n\nFor step 1. I decided to store the `index` of the last tweeted comune in a file\nnamed `last-tweeted.txt`.\nSo this is the file read at the beginning in order to know where to continue from, step 2.,\nor eventually start over again.\n\nStep 3. is about setting some message and credits, but sparingly so as not to annoy\n(i.e. spam) too much the relevant communities.\n\nFinally step 4 is about posting the tweet, followed by writing out the index of the\ncomune.\n\n\n# ToDo's\n \n I am now at the stage where I can execute the 4 steps above and tweet a new map.\n I can execute all the steps on the command line by issuing\n \n\n::: {.cell}\n\n```{.bash .cell-code}\n$ Rscript tweet-bot.R\n```\n:::\n\n \nThis can easily put into a cron job and, for example, tweet\na map every hour (so completing the job in 333 days and 6 hours).\nBut to be frank the code is not robust enough to run without supervision.\n\nWhat I would really like would be to\n\n* use some serverless service like AWS Lambda to trigger the tweeting every hour\n* a robust way to keep state (i.e. what can I use to know what was last tweeted comune's\n  index?)\n* a robust way to cope with failures to post the tweet (i.e. what to do in case of\n  no WIFI/Internet connection)\n\nIf you have any suggestions or comments on the above, feel free to get in touch with me!\n\n\n[gis2016wsg84]: https://www.istat.it/storage/cartografia/confini_amministrativi/archivio-confini/non_generalizzati/2016/Limiti_2016_WGS84.zip\n\n[twitter-bot-repo]: https://github.com/espinielli/italian-comuni-bot/\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}